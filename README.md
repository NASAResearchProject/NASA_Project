# NASA_Project

## Presentation  
Our final report may be found [here](https://docs.google.com/presentation/d/1IT_PiYN4WrnY3WbVbrXUzG-RIjo1Cg-IqWg3Rjlhg8o/edit?usp=sharing).  

## Communication Protocols  
### Week 1  
My group mate Andres and I chose to create a project regarding space related research. The objective is to create and communicate machine learning algorithms that can predict the type of celestial object based on a number of features. There were four aspects we had to cover, the machine learning model, maintaining the repository, database management and data visualizations. Since there are two of us, we split the responsibilities in half. I cover the machine learning and data visualization aspect, and Andres covers the rest. We communicated our expectations and set checkpoints to have done by the weekend. We then met one on one on zoom to discuss where we're at and what trouble we ran into. For instance, some of the column names in the datasets are keywords in SQL, so we had to consider changing the column names. Although we have different responsibilities we worked closely together to make sure one's work was compatible with the other. For instance, I had to ensure the datasets were properly extracted and transformed before Andres could load the data into an SQL database. One thing I appreciate about group work is how one has to constantly communicate their ideas and to make sure everyone is on the same page. It encourages organization. Andres and I are in touch on a daily basis regarding where we are at, what we need assistance with, and what work we are yet to complete to meet our objectives.

### Week 2  
Andres and I decided on our objectives as we left for Christmas break. He would continue working on the SQL database while I worked on the exploratory analysis and machine learning models. We reconvined the very next Tuesday and discussed issues that arose. For instance, we found one of our datasets to be corrupt, all of the rows were identical. This was never an issue for the machine learning model, as we made sure to drop all duplicate rows on the merged dataset, but we decided to drop the dataset anyway. We replaced that dataset with a larger dataset that may allow us to construct a more accurate model. We are yet to implement the new dataset, that is our next step. 


## ETL  
We found 4 datasets on Kaggle. The Sloan Digital Sky Survey data offers the public data on space observations. Across the four datasets, we have accumulated over 700 thousand data points. Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar. My team thought this was perfect to build our machine learning models around. We decided to keep the first machine learning model simple, so we only used the three smallest datasets for the first model. The ETL process was split into two parts. The merging and cleaning of the datasets were the first part. After merging the datasets, wenoticed some duplication of rows. We loaded a dataset with over 100 thousand datapoints for the first process. The second part of the ETL process required us to preprocess the complete dataset so that it may be compatible with any machine learning models that we threw at it. First we deleted columns that we considered to be meaningless when it came to predicting a celestial objects class, such as object location and telescope specifications. Then we checked the data types of the remaining columns. The `class_` column defines what type of object the celestial object it in the form of a string, so I used the `LabelEncoder()` method from the `sklearn.preprocessing` library to do so. The resulting dataset looks as so:  
![image](https://user-images.githubusercontent.com/68082808/102816320-d22ba680-439b-11eb-9e09-0444ddb3114d.png)  

We merge datasets [here](https://github.com/NASAResearchProject/NASA_Project/blob/Amir-branch/ETL/ETL.ipynb).  
Our preprocessing code may be found [here](https://github.com/NASAResearchProject/NASA_Project/blob/Amir-branch/ETL/Cleaning%20Complete%20Dataset.ipynb).

## Machine Learning  
For our first model I chose the supervised algorithm known as [K-Nearest Neighbors](https://github.com/NASAResearchProject/NASA_Project/blob/Amir-branch/Machine%20Learning/K_Nearest_Neighbors.ipynb). The algorithm is simple and elegant. It also required no training step, and is very easy to implement. After the preprocessing step, we had a total of 6 independent features, a relatively small number compared to those of other datasets. This means we don't have to concern ourselves with the curse of dimnesionality, because as the number of variables grow, the K-NN algorithm struggles to predict the output of new data. All of these characteristics of the algorithm and our dataset make choosing the K-NN algorithm an easy choice, and it paid off. Our model was able to predict the class of a celestial object with 95% accuracy. Moving forward I would like to consider scaling all features so that they're homogeneous. K-NN works best when features have the same scale. All columns except for `redshift_` seem to be homogeneous, but nonetheless it is worth tinkering with. I would like to add the fourth dataset to the dataframe as well, because the more datapoints the more data the machine has to learn with. This will allow me to use more robust and complex models such as a neural network.

Scaling the features increased the model by 1 percent. A curve is developed on the training and testing data. Training and testing accuracy scores were plotted for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, we  develop an intuition for overfitting and underfitting. The optimal value of The the program reports the following metrics:  
![image](https://user-images.githubusercontent.com/68082808/103450559-4f9dc380-4c86-11eb-977e-28084034c911.png)
