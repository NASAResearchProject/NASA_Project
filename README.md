# NASA_Project

## Presentation  
Our final report may be found [here](https://docs.google.com/presentation/d/1IT_PiYN4WrnY3WbVbrXUzG-RIjo1Cg-IqWg3Rjlhg8o/edit?usp=sharing).  


## ETL  
We found three datasets on Kaggle. The Sloan Digital Sky Survey data offers the public data on space observations. Across the three datasets, we have accumulated over 700 thousand data points. Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar. My team thought this was perfect to build our machine learning models around. We decided to keep the first machine learning model simple, so we only used the smallest datasets for the first model. The ETL process was split into two parts. The merging and cleaning of the datasets were the first part. After merging the datasets, we noticed some duplication of rows. We loaded a dataset with over 100 thousand datapoints for the first process. The second part of the ETL process required us to preprocess the complete dataset so that it may be compatible with any machine learning models that we threw at it. First we deleted columns that we considered to be meaningless when it came to predicting a celestial objects class, such as object location and telescope specifications. Then we checked the data types of the remaining columns. The `class_` column defines what type of object the celestial object it in the form of a string, so I used the `LabelEncoder()` method from the `sklearn.preprocessing` library to do so. The resulting dataset looks as so:  
![image](https://user-images.githubusercontent.com/68082808/102816320-d22ba680-439b-11eb-9e09-0444ddb3114d.png)  

We merge datasets and preprocess [here](https://github.com/NASAResearchProject/NASA_Project/blob/main/ETL/Dataset_cleaning.ipynb).  

## Machine Learning  
For our first model I chose the supervised algorithm known as [K-Nearest Neighbors](https://github.com/NASAResearchProject/NASA_Project/blob/main/Machine%20Learning/PreProccessing_KNN.ipynb). The algorithm is simple and elegant. It also required no training step, and is very easy to implement. After the preprocessing step, we had a total of 6 independent features, a relatively small number compared to those of other datasets. This means we don't have to concern ourselves with the curse of dimnesionality, because as the number of variables grow, the K-NN algorithm struggles to predict the output of new data. All of these characteristics of the algorithm and our dataset make choosing the K-NN algorithm an easy choice, and it paid off. Our model was able to predict the class of a celestial object with 95% accuracy. Moving forward I would like to consider scaling all features so that they're homogeneous. K-NN works best when features have the same scale. All columns except for `redshift_` seem to be homogeneous, but nonetheless it is worth tinkering with. I would like to add the fourth dataset to the dataframe as well, because the more datapoints the more data the machine has to learn with. This will allow me to use more robust and complex models such as a neural network.

Scaling the features increased the model by 1 percent. A curve is developed on the training and testing data. Training and testing accuracy scores were plotted for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, we  develop an intuition for overfitting and underfitting. The optimal value of The the program reports the following metrics:  
![image](https://user-images.githubusercontent.com/68082808/103450559-4f9dc380-4c86-11eb-977e-28084034c911.png)
